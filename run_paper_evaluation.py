# run_paper_evaluation_fixed.py
#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆçš„ä¸è®ºæ–‡ä¸€è‡´çš„è¯„ä¼°è„šæœ¬
"""

import argparse
import time
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import get_config
from model_training.paper_evaluation import PaperEvaluatorFixed

def main():
    parser = argparse.ArgumentParser(description="ä¿®å¤ç‰ˆçš„ä¸è®ºæ–‡ä¸€è‡´çš„è¯„ä¼°")
    parser.add_argument("--scale", choices=["small", "standard"], default="small",
                       help="è¿è¡Œè§„æ¨¡")
    parser.add_argument("--model_path", type=str, required=True,
                       help="å¾®è°ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_model", type=str, default=None,
                       help="åŸºæ¨¡å‹åç§°ï¼ˆå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„base_modelï¼‰")
    parser.add_argument("--benchmarks", nargs="+", 
                       choices=["humaneval", "mbpp", "all"], default=["humaneval"],
                       help="è¯„ä¼°åŸºå‡†")
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_config(args.scale)
    
    # å¦‚æœæŒ‡å®šäº†åŸºæ¨¡å‹ï¼Œåˆ™è¦†ç›–é…ç½®
    if args.base_model:
        config.base_model = args.base_model
    
    print(f"å¼€å§‹ä¿®å¤ç‰ˆçš„ä¸è®ºæ–‡ä¸€è‡´çš„è¯„ä¼° (è§„æ¨¡: {args.scale})...")
    print(f"åŸºæ¨¡å‹: {config.base_model}")
    print(f"å¾®è°ƒæ¨¡å‹: {args.model_path}")
    
    start_time = time.time()
    
    try:
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not Path(args.model_path).exists():
            raise FileNotFoundError(f"å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = PaperEvaluatorFixed(config)
        
        # åŠ è½½åŸºæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹
        print("åŠ è½½åŸºæ¨¡å‹...")
        evaluator.load_base_model(config.base_model)
        print("åŠ è½½å¾®è°ƒæ¨¡å‹...")
        evaluator.load_model(args.model_path)
        
        benchmarks = args.benchmarks
        if "all" in benchmarks:
            benchmarks = ["humaneval", "mbpp"]
        
        all_results = {}
        
        for benchmark in benchmarks:
            print(f"\n{'='*50}")
            print(f"è¯„ä¼°åŸºå‡†: {benchmark}")
            print(f"{'='*50}")
            
            # è¯„ä¼°åŸºæ¨¡å‹
            print(f"\n1. è¯„ä¼°åŸºæ¨¡å‹...")
            if benchmark == "humaneval":
                base_results = evaluator.evaluate_humaneval(use_base_model=True)
            elif benchmark == "mbpp":
                base_results = evaluator.evaluate_mbpp(use_base_model=True)
            else:
                continue
            
            # è¯„ä¼°å¾®è°ƒæ¨¡å‹
            print(f"\n2. è¯„ä¼°å¾®è°ƒæ¨¡å‹...")
            if benchmark == "humaneval":
                fine_tuned_results = evaluator.evaluate_humaneval(use_base_model=False)
            elif benchmark == "mbpp":
                fine_tuned_results = evaluator.evaluate_mbpp(use_base_model=False)
            else:
                continue
            
            # ä¿å­˜å¯¹æ¯”ç»“æœ
            evaluator.save_comparison_results(base_results, fine_tuned_results, benchmark)
            
            all_results[benchmark] = {
                "base": base_results["pass@1"],
                "fine_tuned": fine_tuned_results["pass@1"],
                "improvement": fine_tuned_results["pass@1"] - base_results["pass@1"]
            }
        
        elapsed_time = time.time() - start_time
        print(f"\n{'='*50}")
        print("ğŸ‰ ä¿®å¤ç‰ˆçš„ä¸è®ºæ–‡ä¸€è‡´çš„è¯„ä¼°å®Œæˆ!")
        print(f"æ€»è€—æ—¶: {elapsed_time/60:.2f} åˆ†é’Ÿ")
        print(f"{'='*50}")
        
        # æ‰“å°æœ€ç»ˆæ‘˜è¦
        print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœæ‘˜è¦:")
        for benchmark, results in all_results.items():
            print(f"  {benchmark.upper()}:")
            print(f"    åŸºæ¨¡å‹: {results['base']*100:.2f}%")
            print(f"    å¾®è°ƒæ¨¡å‹: {results['fine_tuned']*100:.2f}%")
            print(f"    æ”¹è¿›: {results['improvement']*100:+.2f}%")
        
        print(f"\nç»“æœä¿å­˜åœ¨: {Path(config.output_dir) / 'paper_evaluation'}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆ
        print("\nğŸ”§ å…·ä½“è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®ä¿åŸºæ¨¡å‹å¯ä»¥è®¿é—®")
        print("3. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å¯ç”¨")
        
        raise

if __name__ == "__main__":
    main()