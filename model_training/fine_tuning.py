import os
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from transformers.trainer_utils import get_last_checkpoint
import json
from tqdm import tqdm
from pathlib import Path
import datasets
import numpy as np

class CodeInstructionDataset(Dataset):
    """ä»£ç æŒ‡ä»¤æ•°æ®é›†"""
    
    def __init__(self, data_path, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # åŠ è½½æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"åŠ è½½äº† {len(self.data)} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬ä»¥æ£€æŸ¥æ ¼å¼
        if len(self.data) > 0:
            print("ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®:", list(self.data[0].keys()))
        
        self.samples = []
        for item in self.data:
            # æ£€æŸ¥æ•°æ®æ ¼å¼
            if "instruction" not in item:
                print(f"è­¦å‘Š: æ ·æœ¬ç¼ºå°‘ 'instruction' é”®: {item.keys()}")
                continue
                
            if "output" not in item:
                print(f"è­¦å‘Š: æ ·æœ¬ç¼ºå°‘ 'output' é”®: {item.keys()}")
                # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„é”®
                if "response" in item:
                    output_content = item["response"]
                elif "code" in item:
                    output_content = item["code"]
                else:
                    print(f"é”™è¯¯: æ ·æœ¬æ²¡æœ‰è¾“å‡ºå†…å®¹ï¼Œè·³è¿‡: {item.keys()}")
                    continue
            else:
                output_content = item["output"]
            
            # æ ¼å¼åŒ–å¯¹è¯
            conversation = [
                {"role": "user", "content": item["instruction"]},
                {"role": "assistant", "content": output_content}
            ]
            
            # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
            text = self.format_conversation(conversation)
            self.samples.append(text)
        
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°é‡: {len(self.samples)}")
    
    def format_conversation(self, conversation):
        """æ ¼å¼åŒ–å¯¹è¯"""
        formatted_text = ""
        for turn in conversation:
            if turn["role"] == "user":
                formatted_text += f"### æŒ‡ä»¤:\n{turn['content']}\n\n"
            elif turn["role"] == "assistant":
                formatted_text += f"### å“åº”:\n{turn['content']}\n\n"
        
        return formatted_text.strip()
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        text = self.samples[idx]
        
        # ç¼–ç æ–‡æœ¬ - å¯ç”¨å¡«å……å’Œæˆªæ–­
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding=False,  # ä¸åœ¨æ•°æ®é›†çº§åˆ«è¿›è¡Œå¡«å……
            max_length=self.max_length,
            return_tensors=None,
            return_attention_mask=True
        )
        
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•´æ•°ï¼Œå¹¶ä¸”æ˜¯å¹³å¦çš„åˆ—è¡¨
        input_ids = [int(x) for x in encoding['input_ids']]
        attention_mask = [int(x) for x in encoding['attention_mask']]
        
        # è¿”å›å¹³å¦çš„åˆ—è¡¨ï¼Œç¡®ä¿æ²¡æœ‰åµŒå¥—ç»“æ„
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': input_ids.copy()  # å¯¹äºå› æœè¯­è¨€å»ºæ¨¡ï¼Œæ ‡ç­¾å°±æ˜¯è¾“å…¥ID
        }


class SmartDataCollator:
    """æ™ºèƒ½æ•°æ®æ•´ç†å™¨ï¼Œå¤„ç†å˜é•¿åºåˆ—å’Œæ•°æ®ç±»å‹é—®é¢˜"""
    
    def __init__(self, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __call__(self, features):
        # æ‰¾åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦
        max_length = max(len(feature['input_ids']) for feature in features)
        
        # é™åˆ¶æœ€å¤§é•¿åº¦ï¼Œé¿å…å†…å­˜é—®é¢˜
        max_length = min(max_length, self.max_length)
        
        batch = {
            'input_ids': [],
            'attention_mask': [],
            'labels': []
        }
        
        for feature in features:
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•´æ•°åˆ—è¡¨
            input_ids = feature['input_ids']
            attention_mask = feature['attention_mask']
            labels = feature['labels']
            
            # å¦‚æœè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œè¿›è¡Œæˆªæ–­
            if len(input_ids) > max_length:
                input_ids = input_ids[:max_length]
                attention_mask = attention_mask[:max_length]
                labels = labels[:max_length]
            
            # å¦‚æœçŸ­äºæœ€å¤§é•¿åº¦ï¼Œè¿›è¡Œå¡«å……
            if len(input_ids) < max_length:
                pad_length = max_length - len(input_ids)
                input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_length
                attention_mask = attention_mask + [0] * pad_length
                labels = labels + [-100] * pad_length  # å¿½ç•¥å¡«å……ä½ç½®çš„æŸå¤±
            
            # è½¬æ¢ä¸ºå¼ é‡
            batch['input_ids'].append(torch.tensor(input_ids, dtype=torch.long))
            batch['attention_mask'].append(torch.tensor(attention_mask, dtype=torch.long))
            batch['labels'].append(torch.tensor(labels, dtype=torch.long))
        
        # å †å å¼ é‡
        batch = {k: torch.stack(v) for k, v in batch.items()}
        return batch


class CodeFineTuner:
    """ä»£ç å¾®è°ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        print("åŠ è½½åŸºç¡€æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model)
        
        # ç¡®ä¿åˆ†è¯å™¨æœ‰å¡«å……token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # æ ¹æ®ç¡¬ä»¶èƒ½åŠ›é€‰æ‹©é€‚å½“çš„ç²¾åº¦
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"æ£€æµ‹åˆ°GPU: {gpu_name}")
            
            # å¯¹äºè¾ƒæ–°çš„GPUï¼ˆå¦‚4090ï¼‰ï¼Œä½¿ç”¨BF16å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
            if "4090" in gpu_name or "A100" in gpu_name or "H100" in gpu_name:
                self.torch_dtype = torch.bfloat16
                print("ä½¿ç”¨BF16ç²¾åº¦")
            else:
                self.torch_dtype = torch.float16
                print("ä½¿ç”¨FP16ç²¾åº¦")
        else:
            self.torch_dtype = torch.float32
            print("ä½¿ç”¨FP32ç²¾åº¦ï¼ˆCPUæ¨¡å¼ï¼‰")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            config.base_model,
            torch_dtype=self.torch_dtype,
            device_map="auto"
        )
        
        print(f"æ¨¡å‹å‚æ•°é‡: {self.model.num_parameters():,}")
    
    def prepare_data(self, data_path):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError(f"è®­ç»ƒæ•°æ®åº”è¯¥æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œä½†å¾—åˆ°çš„æ˜¯ {type(data)}")
        
        print(f"è®­ç»ƒæ•°æ®åŒ…å« {len(data)} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = CodeInstructionDataset(data_path, self.tokenizer, self.config.max_length)
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        if len(dataset) > 10:
            train_size = int(0.95 * len(dataset))
            val_size = len(dataset) - train_size
            train_dataset, val_dataset = torch.utils.data.random_split(
                dataset, [train_size, val_size]
            )
        else:
            print("è­¦å‘Š: æ•°æ®é›†å¤ªå°ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒé›†")
            train_dataset = dataset
            val_dataset = dataset
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        return train_dataset, val_dataset
    
    def fine_tune(self, train_dataset, val_dataset):
        """å¾®è°ƒæ¨¡å‹"""
        print("å¼€å§‹æ¨¡å‹å¾®è°ƒ...")
        
        # è¾“å‡ºç›®å½•
        output_dir = Path(self.config.output_dir) / "fine_tuned_model"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®ç²¾åº¦é€‰æ‹©é€‚å½“çš„è®­ç»ƒå‚æ•°
        if self.torch_dtype == torch.bfloat16:
            fp16 = False
            bf16 = True
        elif self.torch_dtype == torch.float16:
            fp16 = True
            bf16 = False
        else:
            fp16 = False
            bf16 = False
        
        # è®­ç»ƒå‚æ•° - ä¿®å¤æ··åˆç²¾åº¦é—®é¢˜
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            overwrite_output_dir=True,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.eval_batch_size,
            gradient_accumulation_steps=1,
            warmup_ratio=self.config.warmup_ratio,
            learning_rate=self.config.learning_rate,
            weight_decay=0.01,
            logging_dir=str(output_dir / "logs"),
            logging_steps=10,
            eval_strategy="steps",
            eval_steps=50,
            save_strategy="steps",
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            prediction_loss_only=True,
            remove_unused_columns=False,
            ddp_find_unused_parameters=False,
            dataloader_pin_memory=False,
            report_to=[],  # ç¦ç”¨wandbç­‰è®°å½•å™¨
            
            # ä¿®å¤æ··åˆç²¾åº¦è®¾ç½®
            fp16=fp16,
            bf16=bf16,
            tf32=True if not fp16 and not bf16 else False,  # å¯ç”¨TF32ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
            
            dataloader_drop_last=True,
            
            # æ¢¯åº¦è£å‰ªè®¾ç½®
            max_grad_norm=1.0,
            
            # ä¼˜åŒ–å™¨è®¾ç½®
            optim="adamw_torch",
        )
        
        # ä½¿ç”¨æ™ºèƒ½æ•°æ®æ•´ç†å™¨
        data_collator = SmartDataCollator(
            tokenizer=self.tokenizer,
            max_length=self.config.max_length
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            processing_class=self.tokenizer
        )
        
        # è®­ç»ƒå‰æ£€æŸ¥
        if get_last_checkpoint(output_dir) is not None:
            print("å‘ç°æ£€æŸ¥ç‚¹ï¼Œä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ...")
        
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒ...")
        try:
            trainer.train()
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            trainer.save_model()
            self.tokenizer.save_pretrained(output_dir)
            
            print(f"æ¨¡å‹å¾®è°ƒå®Œæˆï¼Œä¿å­˜åœ¨: {output_dir}")
            
            return str(output_dir)
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            # æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆ
            self._handle_training_error(e, trainer, output_dir)
            raise
    
    def _handle_training_error(self, error, trainer, output_dir):
        """å¤„ç†è®­ç»ƒé”™è¯¯"""
        error_msg = str(error)
        
        if "Attempting to unscale FP16 gradients" in error_msg:
            print("\nğŸ”§ æ£€æµ‹åˆ°æ··åˆç²¾åº¦è®­ç»ƒé—®é¢˜ï¼Œå°è¯•ä¿®å¤...")
            print("è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨BF16è€Œä¸æ˜¯FP16ï¼Œæˆ–è€…å®Œå…¨ç¦ç”¨æ··åˆç²¾åº¦")
            
            # å°è¯•ä¿å­˜å½“å‰è¿›åº¦
            try:
                trainer.save_model()
                print(f"å·²ä¿å­˜å½“å‰è¿›åº¦åˆ°: {output_dir}")
            except:
                print("æ— æ³•ä¿å­˜å½“å‰è¿›åº¦")
        
        elif "CUDA out of memory" in error_msg:
            print("\nğŸ”§ æ£€æµ‹åˆ°æ˜¾å­˜ä¸è¶³é—®é¢˜")
            print("è§£å†³æ–¹æ¡ˆ:")
            print("1. å‡å°‘æ‰¹é‡å¤§å°")
            print("2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯")
            print("3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            print("4. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
        else:
            print("\nğŸ”§ ä¸€èˆ¬æ€§è®­ç»ƒé”™è¯¯")
            print("è¯·æ£€æŸ¥:")
            print("1. æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®")
            print("2. æ¨¡å‹é…ç½®æ˜¯å¦å…¼å®¹")
            print("3. ç¡¬ä»¶èµ„æºæ˜¯å¦å……è¶³")
    
    def save_training_info(self, train_dataset, val_dataset, output_dir):
        """ä¿å­˜è®­ç»ƒä¿¡æ¯"""
        info = {
            "base_model": self.config.base_model,
            "train_samples": len(train_dataset),
            "val_samples": len(val_dataset),
            "batch_size": self.config.batch_size,
            "learning_rate": self.config.learning_rate,
            "epochs": self.config.num_epochs,
            "max_length": self.config.max_length,
            "precision": str(self.torch_dtype)
        }
        
        info_file = Path(output_dir) / "training_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)