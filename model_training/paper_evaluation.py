import torch
import datetime  # æ·»åŠ  datetime å¯¼å…¥
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import json
from tqdm import tqdm
from pathlib import Path
import sys
import os
import subprocess
import tempfile
import ast
import signal

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Execution timed out")

class PaperEvaluatorFixed:
    """ä¿®å¤ç‰ˆçš„è®ºæ–‡è¯„ä¼°å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_base_model(self, model_name):
        """åŠ è½½æœªå¾®è°ƒçš„åŸºæ¨¡å‹"""
        print(f"åŠ è½½åŸºæ¨¡å‹: {model_name}")
        self.base_tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        if self.base_tokenizer.pad_token is None:
            self.base_tokenizer.pad_token = self.base_tokenizer.eos_token
        
        print("åŸºæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def evaluate_humaneval(self, use_base_model=False):
        """åœ¨HumanEvalä¸Šè¯„ä¼°"""
        print("åœ¨HumanEvalä¸Šè¯„ä¼°...")
        
        try:
            dataset = load_dataset("openai/openai_humaneval", split="test")
            return self._evaluate_humaneval_dataset(dataset, use_base_model)
        except Exception as e:
            print(f"HumanEvalè¯„ä¼°å¤±è´¥: {e}")
            return self._evaluate_mock_humaneval(use_base_model)
    
    def evaluate_mbpp(self, use_base_model=False):
        """åœ¨MBPPä¸Šè¯„ä¼° - ä¿®å¤å­—æ®µé—®é¢˜"""
        print("åœ¨MBPPä¸Šè¯„ä¼°...")
        
        try:
            # å°è¯•ä¸åŒçš„MBPPæ•°æ®é›†åŠ è½½æ–¹å¼
            try:
                dataset = load_dataset("mbpp", "sanitized", split="test")
            except:
                dataset = load_dataset("mbpp", split="test")
            
            return self._evaluate_mbpp_dataset_fixed(dataset, use_base_model)
        except Exception as e:
            print(f"MBPPè¯„ä¼°å¤±è´¥: {e}")
            return self._evaluate_mock_mbpp(use_base_model)
    
    def _evaluate_humaneval_dataset(self, dataset, use_base_model=False):
        """è¯„ä¼°HumanEvalæ•°æ®é›†"""
        results = []
        total = min(self.config.max_eval_samples, len(dataset)) if self.config.max_eval_samples > 0 else len(dataset)
        
        for i in tqdm(range(total), desc=f"è¯„ä¼°HumanEval ({'åŸºæ¨¡å‹' if use_base_model else 'å¾®è°ƒæ¨¡å‹'})"):
            example = dataset[i]
            prompt = example["prompt"]
            canonical_solution = example["canonical_solution"]
            test_cases = example["test"]
            task_id = example["task_id"]
            
            # ç”Ÿæˆä»£ç 
            if use_base_model:
                generated_code = self._generate_code_with_model(prompt, self.base_model, self.base_tokenizer)
            else:
                generated_code = self._generate_code_with_model(prompt, self.model, self.tokenizer)
            
            # æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
            passed = self._execute_test_cases(generated_code, test_cases, canonical_solution)
            
            results.append({
                "task_id": task_id,
                "passed": passed,
                "generated_code": generated_code,
                "canonical_solution": canonical_solution
            })
        
        # è®¡ç®—pass@1
        pass_count = sum(1 for r in results if r["passed"])
        pass_rate = pass_count / len(results) if results else 0
        
        return {
            "dataset": "humaneval",
            "model_type": "base" if use_base_model else "fine_tuned",
            "pass@1": pass_rate,
            "total_samples": len(results),
            "passed_samples": pass_count,
            "detailed_results": results
        }
    
    def _evaluate_mbpp_dataset_fixed(self, dataset, use_base_model=False):
        """ä¿®å¤ç‰ˆçš„MBPPæ•°æ®é›†è¯„ä¼°"""
        results = []
        total = min(self.config.max_eval_samples, len(dataset)) if self.config.max_eval_samples > 0 else len(dataset)
        
        for i in tqdm(range(total), desc=f"è¯„ä¼°MBPP ({'åŸºæ¨¡å‹' if use_base_model else 'å¾®è°ƒæ¨¡å‹'})"):
            example = dataset[i]
            
            # ä¿®å¤ï¼šæ£€æŸ¥å¹¶é€‚é…ä¸åŒçš„å­—æ®µå
            if "text" in example:
                prompt = example["text"]
            elif "prompt" in example:
                prompt = example["prompt"]
            elif "description" in example:
                prompt = example["description"]
            else:
                # æ‰“å°å¯ç”¨çš„å­—æ®µä»¥ä¾¿è°ƒè¯•
                print(f"å¯ç”¨çš„å­—æ®µ: {list(example.keys())}")
                prompt = str(example)  # å›é€€æ–¹æ¡ˆ
            
            if "code" in example:
                canonical_solution = example["code"]
            elif "canonical_solution" in example:
                canonical_solution = example["canonical_solution"]
            else:
                canonical_solution = ""
            
            if "test_list" in example:
                test_list = example["test_list"]
            elif "test" in example:
                test_list = example["test"]
            else:
                test_list = []
            
            task_id = example.get("task_id", f"mbpp_{i}")
            
            # ç”Ÿæˆä»£ç 
            if use_base_model:
                generated_code = self._generate_code_with_model(prompt, self.base_model, self.base_tokenizer)
            else:
                generated_code = self._generate_code_with_model(prompt, self.model, self.tokenizer)
            
            # æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
            passed = self._execute_mbpp_test_cases(generated_code, test_list, canonical_solution)
            
            results.append({
                "task_id": task_id,
                "passed": passed,
                "generated_code": generated_code,
                "canonical_solution": canonical_solution
            })
        
        # è®¡ç®—pass@1
        pass_count = sum(1 for r in results if r["passed"])
        pass_rate = pass_count / len(results) if results else 0
        
        return {
            "dataset": "mbpp",
            "model_type": "base" if use_base_model else "fine_tuned",
            "pass@1": pass_rate,
            "total_samples": len(results),
            "passed_samples": pass_count,
            "detailed_results": results
        }
    
    def _generate_code_with_model(self, prompt, model, tokenizer):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆä»£ç """
        # æ ¼å¼åŒ–æç¤º - ä¸è®ºæ–‡ä¸­çš„æ ¼å¼ä¸€è‡´
        formatted_prompt = f"### æŒ‡ä»¤:\n{prompt}\n\n### å“åº”:\n"
        
        inputs = tokenizer(formatted_prompt, return_tensors="pt", max_length=1024, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.2,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆçš„ä»£ç éƒ¨åˆ†
        if "### å“åº”:" in generated_text:
            code = generated_text.split("### å“åº”:")[-1].strip()
        else:
            code = generated_text[len(formatted_prompt):].strip()
        
        return code
    
    def _execute_test_cases(self, generated_code, test_cases, canonical_solution):
        """æ‰§è¡ŒHumanEvalæµ‹è¯•ç”¨ä¾‹"""
        try:
            # åˆ›å»ºå®Œæ•´çš„Pythonä»£ç 
            full_code = generated_code + "\n\n" + test_cases
            
            # åœ¨éš”ç¦»ç¯å¢ƒä¸­æ‰§è¡Œ
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(full_code)
                temp_file = f.name
            
            try:
                # è®¾ç½®è¶…æ—¶ï¼ˆ5ç§’ï¼‰
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(5)
                
                # æ‰§è¡Œä»£ç 
                result = subprocess.run(
                    [sys.executable, temp_file],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                
                # æ£€æŸ¥æ‰§è¡Œç»“æœ
                return result.returncode == 0
                    
            except TimeoutException:
                return False
            except subprocess.TimeoutExpired:
                return False
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
                
        except Exception:
            return False
    
    def _execute_mbpp_test_cases(self, generated_code, test_list, canonical_solution):
        """æ‰§è¡ŒMBPPæµ‹è¯•ç”¨ä¾‹"""
        try:
            # åˆ›å»ºå®Œæ•´çš„Pythonä»£ç 
            test_code = "\n".join([f"    {test}" for test in test_list])
            full_code = f"""
{generated_code}

def test_function():
{test_code}

if __name__ == "__main__":
    test_function()
    print("All tests passed!")
"""
            
            # åœ¨éš”ç¦»ç¯å¢ƒä¸­æ‰§è¡Œ
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(full_code)
                temp_file = f.name
            
            try:
                # è®¾ç½®è¶…æ—¶ï¼ˆ5ç§’ï¼‰
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(5)
                
                # æ‰§è¡Œä»£ç 
                result = subprocess.run(
                    [sys.executable, temp_file],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                
                # æ£€æŸ¥æ‰§è¡Œç»“æœ
                return result.returncode == 0 and "All tests passed!" in result.stdout
                    
            except TimeoutException:
                return False
            except subprocess.TimeoutExpired:
                return False
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
                
        except Exception:
            return False
    
    def _evaluate_mock_humaneval(self, use_base_model=False):
        """æ¨¡æ‹ŸHumanEvalè¯„ä¼°"""
        print("ä½¿ç”¨æ¨¡æ‹ŸHumanEvalæ•°æ®...")
        
        mock_data = [
            {
                "task_id": "mock_1",
                "prompt": "def add(a, b):\n    \"\"\"è¿”å›ä¸¤ä¸ªæ•°å­—çš„å’Œ\"\"\"\n    ",
                "canonical_solution": "return a + b",
                "test": "assert add(1, 2) == 3\nassert add(0, 0) == 0\nassert add(-1, 1) == 0"
            }
        ]
        
        results = []
        for example in mock_data:
            prompt = example["prompt"]
            canonical_solution = example["canonical_solution"]
            test_cases = example["test"]
            task_id = example["task_id"]
            
            if use_base_model:
                generated_code = self._generate_code_with_model(prompt, self.base_model, self.base_tokenizer)
            else:
                generated_code = self._generate_code_with_model(prompt, self.model, self.tokenizer)
            
            # ç®€å•æ£€æŸ¥ç”Ÿæˆçš„ä»£ç æ˜¯å¦åŒ…å«å…³é”®éƒ¨åˆ†
            passed = "return" in generated_code and ("a + b" in generated_code or "a+b" in generated_code)
            
            results.append({
                "task_id": task_id,
                "passed": passed,
                "generated_code": generated_code,
                "canonical_solution": canonical_solution
            })
        
        pass_count = sum(1 for r in results if r["passed"])
        pass_rate = pass_count / len(results) if results else 0
        
        return {
            "dataset": "humaneval_mock",
            "model_type": "base" if use_base_model else "fine_tuned",
            "pass@1": pass_rate,
            "total_samples": len(results),
            "passed_samples": pass_count,
            "detailed_results": results
        }
    
    def _evaluate_mock_mbpp(self, use_base_model=False):
        """æ¨¡æ‹ŸMBPPè¯„ä¼°"""
        print("ä½¿ç”¨æ¨¡æ‹ŸMBPPæ•°æ®...")
        
        mock_data = [
            {
                "task_id": 1,
                "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—ä¸¤ä¸ªæ•°å­—ä½œä¸ºè¾“å…¥å¹¶è¿”å›å®ƒä»¬çš„å’Œ",
                "code": "def add(a, b):\n    return a + b",
                "test_list": ["assert add(1, 2) == 3", "assert add(0, 0) == 0", "assert add(-1, 1) == 0"]
            }
        ]
        
        results = []
        for example in mock_data:
            prompt = example["text"]
            canonical_solution = example["code"]
            test_list = example["test_list"]
            task_id = example["task_id"]
            
            if use_base_model:
                generated_code = self._generate_code_with_model(prompt, self.base_model, self.base_tokenizer)
            else:
                generated_code = self._generate_code_with_model(prompt, self.model, self.tokenizer)
            
            # ç®€å•æ£€æŸ¥ç”Ÿæˆçš„ä»£ç æ˜¯å¦åŒ…å«å…³é”®éƒ¨åˆ†
            passed = "return" in generated_code and ("a + b" in generated_code or "a+b" in generated_code)
            
            results.append({
                "task_id": task_id,
                "passed": passed,
                "generated_code": generated_code,
                "canonical_solution": canonical_solution
            })
        
        pass_count = sum(1 for r in results if r["passed"])
        pass_rate = pass_count / len(results) if results else 0
        
        return {
            "dataset": "mbpp_mock",
            "model_type": "base" if use_base_model else "fine_tuned",
            "pass@1": pass_rate,
            "total_samples": len(results),
            "passed_samples": pass_count,
            "detailed_results": results
        }
    
    def save_comparison_results(self, base_results, fine_tuned_results, benchmark_name):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        output_dir = Path(self.config.output_dir) / "paper_evaluation"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå¯¹æ¯”ç»“æœ
        comparison = {
            "benchmark": benchmark_name,
            "base_model": self.config.base_model,
            "fine_tuned_model": "custom_fine_tuned",
            "results": {
                "base_model": {
                    "pass@1": base_results["pass@1"],
                    "total_samples": base_results["total_samples"],
                    "passed_samples": base_results["passed_samples"]
                },
                "fine_tuned_model": {
                    "pass@1": fine_tuned_results["pass@1"],
                    "total_samples": fine_tuned_results["total_samples"],
                    "passed_samples": fine_tuned_results["passed_samples"]
                },
                "improvement": fine_tuned_results["pass@1"] - base_results["pass@1"]
            },
            "timestamp": str(datetime.datetime.now())  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ datetime
        }
        
        output_file = output_dir / f"{benchmark_name}_comparison.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = output_dir / f"{benchmark_name}_detailed.json"
        detailed_results = {
            "base_model_detailed": base_results["detailed_results"],
            "fine_tuned_model_detailed": fine_tuned_results["detailed_results"]
        }
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        print(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ‰“å°æ‘˜è¦
        print(f"\nğŸ“Š {benchmark_name.upper()} è¯„ä¼°ç»“æœå¯¹æ¯”:")
        print(f"  åŸºæ¨¡å‹ ({self.config.base_model}): {base_results['pass@1']*100:.2f}% pass@1")
        print(f"  å¾®è°ƒæ¨¡å‹: {fine_tuned_results['pass@1']*100:.2f}% pass@1")
        print(f"  æ”¹è¿›: {comparison['results']['improvement']*100:+.2f}%")