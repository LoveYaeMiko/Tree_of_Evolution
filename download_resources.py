import os
import requests
import zipfile
import gzip
import shutil
from pathlib import Path
from tqdm import tqdm
from huggingface_hub import snapshot_download, hf_hub_download
import datasets
import json

class ResourceDownloader:
    def __init__(self, base_dir="./resources"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
    def download_stack_v1(self):
        """ä¸‹è½½Stack v1æ•°æ®é›†"""
        print("ä¸‹è½½Stack v1æ•°æ®é›†...")
        stack_dir = self.base_dir / "stack_v1"
        stack_dir.mkdir(exist_ok=True)
        
        # ä½¿ç”¨Hugging Face datasetsä¸‹è½½
        try:
            from datasets import load_dataset
            # ä½¿ç”¨streamingæ¨¡å¼ä¸‹è½½éƒ¨åˆ†æ•°æ®
            dataset = load_dataset("bigcode/the-stack", 
                                 data_dir="data/python", 
                                 split="train", 
                                 streaming=True,
                                 trust_remote_code=True)
            
            # å–å‰5000ä¸ªæ ·æœ¬ä½œä¸ºç§å­
            samples = []
            for i, sample in enumerate(dataset):
                if i >= 5000:
                    break
                if i % 1000 == 0:
                    print(f"å·²ä¸‹è½½ {i} ä¸ªæ ·æœ¬...")
                samples.append(sample)
                
            # ä¿å­˜ä¸ºJSON
            with open(stack_dir / "seed_samples.json", "w") as f:
                json.dump(samples, f, indent=2)
                
            print(f"âœ“ Stack v1æ•°æ®é›†ä¸‹è½½å®Œæˆï¼Œå…±{len(samples)}ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"ä¸‹è½½Stack v1å¤±è´¥: {e}")
            print("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            self._create_mock_stack_data(stack_dir)
    
    def _create_mock_stack_data(self, stack_dir):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„Stackæ•°æ®"""
        mock_data = []
        
        # åˆ›å»ºä¸€äº›çœŸå®çš„Pythonä»£ç ç¤ºä¾‹ä½œä¸ºç§å­
        sample_codes = [
            """
def fibonacci(n):
    \"\"\"è®¡ç®—ç¬¬nä¸ªæ–æ³¢é‚£å¥‘æ•°\"\"\"
    if n <= 1:
        return n
    a, b = 0, 1
    for i in range(2, n + 1):
        a, b = b, a + b
    return b
            """,
            """
def factorial(n):
    \"\"\"è®¡ç®—nçš„é˜¶ä¹˜\"\"\"
    if n == 0:
        return 1
    result = 1
    for i in range(1, n + 1):
        result *= i
    return result
            """,
            """
def is_prime(n):
    \"\"\"æ£€æŸ¥æ•°å­—æ˜¯å¦ä¸ºè´¨æ•°\"\"\"
    if n <= 1:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True
            """,
            """
def binary_search(arr, target):
    \"\"\"äºŒåˆ†æŸ¥æ‰¾ç®—æ³•\"\"\"
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
            """,
            """
def quick_sort(arr):
    \"\"\"å¿«é€Ÿæ’åºç®—æ³•\"\"\"
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)
            """
        ]
        
        for i, code in enumerate(sample_codes):
            mock_data.append({
                "content": code.strip(),
                "size": len(code),
                "license": "mit",
                "file_name": f"sample_code_{i}.py"
            })
        
        # æ·»åŠ æ›´å¤šæ¨¡æ‹Ÿæ•°æ®
        for i in range(len(sample_codes), 100):
            mock_data.append({
                "content": f"def example_function_{i}():\n    return 'This is sample code {i}'",
                "size": 100 + i,
                "license": "mit",
                "file_name": f"example_{i}.py"
            })
        
        with open(stack_dir / "seed_samples.json", "w") as f:
            json.dump(mock_data, f, indent=2)
        print("âœ“ åˆ›å»ºäº†æ¨¡æ‹ŸStackæ•°æ®")
    
    def download_benchmarks(self):
        """ä¸‹è½½è¯„ä¼°åŸºå‡†æ•°æ®é›†"""
        print("ä¸‹è½½è¯„ä¼°åŸºå‡†æ•°æ®é›†...")
        benchmarks_dir = self.base_dir / "benchmarks"
        benchmarks_dir.mkdir(exist_ok=True)
        
        # HumanEval
        try:
            print("ä¸‹è½½HumanEvalæ•°æ®é›†...")
            from datasets import load_dataset
            # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†åç§°
            humaneval = load_dataset("openai/openai_humaneval", trust_remote_code=True)
            
            # å°†è·¯å¾„è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            humaneval.save_to_disk(str(benchmarks_dir / "humaneval"))
            print("âœ“ HumanEvalæ•°æ®é›†ä¸‹è½½å®Œæˆ")
            
            # åŒæ—¶ä¿å­˜ä¸ºJSONæ ¼å¼ä¾¿äºæŸ¥çœ‹
            test_data = humaneval["test"]
            with open(benchmarks_dir / "humaneval_test.json", "w") as f:
                json.dump(test_data.to_dict(), f, indent=2)
                
        except Exception as e:
            print(f"ä¸‹è½½HumanEvalå¤±è´¥: {e}")
            print("åˆ›å»ºæ¨¡æ‹ŸHumanEvalæ•°æ®...")
            self._create_mock_humaneval(benchmarks_dir)
            
        # MBPP
        try:
            print("ä¸‹è½½MBPPæ•°æ®é›†...")
            from datasets import load_dataset
            # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†åç§°
            mbpp = load_dataset("mbpp", "sanitized", trust_remote_code=True)  # ä½¿ç”¨sanitizedç‰ˆæœ¬
            
            # å°†è·¯å¾„è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            mbpp.save_to_disk(str(benchmarks_dir / "mbpp"))
            print("âœ“ MBPPæ•°æ®é›†ä¸‹è½½å®Œæˆ")
            
            # åŒæ—¶ä¿å­˜ä¸ºJSONæ ¼å¼ä¾¿äºæŸ¥çœ‹
            test_data = mbpp["test"]
            with open(benchmarks_dir / "mbpp_test.json", "w") as f:
                json.dump(test_data.to_dict(), f, indent=2)
                
        except Exception as e:
            print(f"ä¸‹è½½MBPPå¤±è´¥: {e}")
            print("åˆ›å»ºæ¨¡æ‹ŸMBPPæ•°æ®...")
            self._create_mock_mbpp(benchmarks_dir)
    
    def _create_mock_humaneval(self, benchmarks_dir):
        """åˆ›å»ºæ¨¡æ‹ŸHumanEvalæ•°æ®"""
        humaneval_dir = benchmarks_dir / "humaneval"
        humaneval_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„HumanEvalæµ‹è¯•æ•°æ®
        mock_humaneval = {
            "test": [
                {
                    "task_id": "test_0",
                    "prompt": "def add(a, b):\n    \"\"\"è¿”å›ä¸¤ä¸ªæ•°å­—çš„å’Œ\"\"\"\n    ",
                    "canonical_solution": "return a + b",
                    "test": "assert add(1, 2) == 3\nassert add(0, 0) == 0\nassert add(-1, 1) == 0",
                    "entry_point": "add"
                },
                {
                    "task_id": "test_1", 
                    "prompt": "def factorial(n):\n    \"\"\"è®¡ç®—nçš„é˜¶ä¹˜\"\"\"\n    ",
                    "canonical_solution": "if n == 0:\n    return 1\nelse:\n    return n * factorial(n-1)",
                    "test": "assert factorial(0) == 1\nassert factorial(1) == 1\nassert factorial(5) == 120",
                    "entry_point": "factorial"
                },
                {
                    "task_id": "test_2",
                    "prompt": "def is_palindrome(s):\n    \"\"\"æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦æ˜¯å›æ–‡\"\"\"\n    ",
                    "canonical_solution": "return s == s[::-1]",
                    "test": "assert is_palindrome('racecar') == True\nassert is_palindrome('hello') == False\nassert is_palindrome('a') == True",
                    "entry_point": "is_palindrome"
                }
            ]
        }
        
        # ä¿å­˜æ¨¡æ‹Ÿæ•°æ®
        with open(humaneval_dir / "dataset_dict.json", "w") as f:
            json.dump(mock_humaneval, f, indent=2)
        
        with open(benchmarks_dir / "humaneval_test.json", "w") as f:
            json.dump(mock_humaneval["test"], f, indent=2)
            
        print("âœ“ åˆ›å»ºäº†æ¨¡æ‹ŸHumanEvalæ•°æ®")
    
    def _create_mock_mbpp(self, benchmarks_dir):
        """åˆ›å»ºæ¨¡æ‹ŸMBPPæ•°æ®"""
        mbpp_dir = benchmarks_dir / "mbpp"
        mbpp_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„MBPPæµ‹è¯•æ•°æ®
        mock_mbpp = {
            "test": [
                {
                    "task_id": 1,
                    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—ä¸¤ä¸ªæ•°å­—ä½œä¸ºè¾“å…¥å¹¶è¿”å›å®ƒä»¬çš„å’Œ",
                    "code": "def add(a, b):\n    return a + b",
                    "test_list": ["assert add(1, 2) == 3", "assert add(0, 0) == 0", "assert add(-1, 1) == 0"]
                },
                {
                    "task_id": 2,
                    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°è®¡ç®—åˆ—è¡¨ä¸­æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼",
                    "code": "def average(lst):\n    return sum(lst) / len(lst) if lst else 0",
                    "test_list": ["assert average([1, 2, 3]) == 2.0", "assert average([]) == 0", "assert average([5]) == 5.0"]
                },
                {
                    "task_id": 3,
                    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦æ˜¯å›æ–‡",
                    "code": "def is_palindrome(s):\n    return s == s[::-1]",
                    "test_list": ["assert is_palindrome('racecar') == True", "assert is_palindrome('hello') == False", "assert is_palindrome('a') == True"]
                }
            ]
        }
        
        # ä¿å­˜æ¨¡æ‹Ÿæ•°æ®
        with open(mbpp_dir / "dataset_dict.json", "w") as f:
            json.dump(mock_mbpp, f, indent=2)
        
        with open(benchmarks_dir / "mbpp_test.json", "w") as f:
            json.dump(mock_mbpp["test"], f, indent=2)
            
        print("âœ“ åˆ›å»ºäº†æ¨¡æ‹ŸMBPPæ•°æ®")
    
    def download_models(self):
        """ä¸‹è½½æ¨¡å‹"""
        print("ä¸‹è½½æ¨¡å‹...")
        models_dir = self.base_dir / "models"
        models_dir.mkdir(exist_ok=True)
        
        model_configs = {
            "qwen2.5-coder-1.5b": "Qwen/Qwen2.5-Coder-1.5B",
            "qwen2.5-coder-7b": "Qwen/Qwen2.5-Coder-7B", 
            "gte-large": "thenlper/gte-large"
        }
        
        for model_name, hf_path in model_configs.items():
            model_dir = models_dir / model_name
            if model_dir.exists():
                # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½äº†å…³é”®æ–‡ä»¶
                required_files = ["config.json", "pytorch_model.bin", "model.safetensors"]
                has_files = any((model_dir / f).exists() for f in required_files)
                
                if has_files:
                    print(f"âœ“ {model_name} å·²å­˜åœ¨")
                    continue
                else:
                    print(f"âš  {model_name} ç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½...")
                    shutil.rmtree(model_dir)
                
            try:
                print(f"ä¸‹è½½ {model_name}...")
                snapshot_download(
                    repo_id=hf_path,
                    local_dir=str(model_dir),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    local_dir_use_symlinks=False,
                    resume_download=True
                )
                print(f"âœ“ {model_name} ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"ä¸‹è½½ {model_name} å¤±è´¥: {e}")
                print(f"è¯·æ‰‹åŠ¨ä¸‹è½½: {hf_path}")
    
    def download_all(self):
        """ä¸‹è½½æ‰€æœ‰èµ„æº"""
        print("å¼€å§‹ä¸‹è½½æ‰€æœ‰å¿…è¦èµ„æº...")
        self.download_stack_v1()
        self.download_benchmarks() 
        self.download_models()
        print("âœ“ æ‰€æœ‰èµ„æºä¸‹è½½å®Œæˆ!")
        
        # æ˜¾ç¤ºä¸‹è½½æ‘˜è¦
        self.show_download_summary()
    
    def show_download_summary(self):
        """æ˜¾ç¤ºä¸‹è½½æ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“Š ä¸‹è½½æ‘˜è¦")
        print("="*50)
        
        # æ£€æŸ¥Stack v1
        stack_file = self.base_dir / "stack_v1" / "seed_samples.json"
        if stack_file.exists():
            with open(stack_file, 'r') as f:
                data = json.load(f)
            print(f"Stack v1: {len(data)} ä¸ªä»£ç æ ·æœ¬")
        else:
            print("Stack v1: âœ— ä¸‹è½½å¤±è´¥")
        
        # æ£€æŸ¥åŸºå‡†æ•°æ®é›†
        benchmarks = ["humaneval", "mbpp"]
        for benchmark in benchmarks:
            benchmark_dir = self.base_dir / "benchmarks" / benchmark
            if benchmark_dir.exists():
                try:
                    with open(self.base_dir / "benchmarks" / f"{benchmark}_test.json", 'r') as f:
                        data = json.load(f)
                    print(f"{benchmark.upper()}: {len(data)} ä¸ªæµ‹è¯•æ ·æœ¬")
                except:
                    print(f"{benchmark.upper()}: âœ“ å·²ä¸‹è½½")
            else:
                print(f"{benchmark.upper()}: âœ— ä¸‹è½½å¤±è´¥")
        
        # æ£€æŸ¥æ¨¡å‹
        models_dir = self.base_dir / "models"
        for model_dir in models_dir.iterdir():
            if model_dir.is_dir():
                files = list(model_dir.glob("*"))
                print(f"{model_dir.name}: {len(files)} ä¸ªæ–‡ä»¶")
        
        print("="*50)

if __name__ == "__main__":
    downloader = ResourceDownloader()
    downloader.download_all()